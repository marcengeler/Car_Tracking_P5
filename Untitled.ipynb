{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Tracking\n",
    "\n",
    "This project copes with the identification of cars in a video. To do so, several functions are used to extract features from an image. Those functions can e.g. be:\n",
    "\n",
    "* Color Histogram\n",
    "* Gradient Information\n",
    "* Spatial Binning\n",
    "\n",
    "and many more. Also, using different color spaces of images can generate a feature space. With that feature space, a classifier can be identified, that tries to separate car from non car images.\n",
    "In the end this classifier can be used to identify cars on images. Using filtering, false positives can be filtered out, and true positives can be brought down to a bounding box calculation, which can be used to predict the future position of the car in the image.\n",
    "\n",
    "## Feature Extraction\n",
    "\n",
    "To start out on this project, different approaches to feature extraction are tested. This way, we can identify useful features, which can be given to a classifier to identify car images, and to differ them from non car images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Database Reading\n",
    "\n",
    "We start off by importing the image database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import os component to do the file handling\n",
    "from os import listdir, walk\n",
    "from os.path import isfile, isdir, join\n",
    "\n",
    "## Import OpenCV to do the image reading\n",
    "import cv2\n",
    "\n",
    "\n",
    "## Import numpy to convert images to numpy arrays\n",
    "import numpy as np\n",
    "\n",
    "veh_dir = '/user_storage/meng/data/vehicles'\n",
    "#veh_dir = \"C:/Users/Marc Engeler/PycharmProjects/car_tracking_training/vehicles\"\n",
    "vehicle_folders = [join(veh_dir, name) for name in listdir(veh_dir) if isdir(join(veh_dir, name))]\n",
    "\n",
    "nveh_dir = '/user_storage/meng/data/non-vehicles'\n",
    "#nveh_dir = \"C:/Users/Marc Engeler/PycharmProjects/car_tracking_training/non-vehicles\"\n",
    "non_vehicle_folders = [join(nveh_dir, name) for name in listdir(nveh_dir) if isdir(join(nveh_dir, name))]\n",
    "\n",
    "\n",
    "train_files = []\n",
    "for vehicle_folder in vehicle_folders:\n",
    "    new_files = [join(vehicle_folder, f) for f in listdir(vehicle_folder) if isfile(join(vehicle_folder, f))]\n",
    "    for file in new_files:\n",
    "        train_files.append(file)\n",
    "        \n",
    "for vehicle_folder in non_vehicle_folders:\n",
    "    new_files = [join(vehicle_folder, f) for f in listdir(vehicle_folder) if isfile(join(vehicle_folder, f))]\n",
    "    for file in new_files:\n",
    "        train_files.append(file)\n",
    "\n",
    "y_train = []\n",
    "x_train = []\n",
    "for train_file in train_files:\n",
    "    if 'non-vehicle' in train_file:\n",
    "        y_train.append(0)\n",
    "    else:\n",
    "        y_train.append(1)\n",
    "    image = cv2.imread(train_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    x_train.append(np.uint8(image))\n",
    "    \n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, we want to run some sanity and balancing checks. Is a class overrepresented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib to plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "count = np.bincount(y_train)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(np.unique(y_train), count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that both classes are equally represented, which is a good starting point, so we don't have to generate additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_rand = np.random.uniform(0, x_train.shape[0], 10)\n",
    "\n",
    "fig, ax = plt.subplots(2,5)\n",
    "for i, k in enumerate(index_rand):\n",
    "    img = x_train[int(k)]\n",
    "    ax[i%2, int(i/2)].imshow(img)\n",
    "    ax[i%2, int(i/2)].set_title(y_train[int(k)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random check on the dataset shows, that everything seems to be in order with the inport. Classes are assigned as 1 for vehicle and 0 for non-vehicle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "To extract features from the images, I started off with the principles learned in the course.\n",
    "\n",
    "The first approach taken was the HOG extraction, which is coming with the scikit library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "orient = 9\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "\n",
    "index_rand = np.random.uniform(0, x_train.shape[0], 10)\n",
    "fig, ax = plt.subplots(4,5, figsize = (12,8))\n",
    "for i, k in enumerate(index_rand):\n",
    "    img = x_train[int(k)]\n",
    "    feature_array, hogimg = hog(img[:,:,0], orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell), cells_per_block=(cell_per_block, cell_per_block), visualise=True, feature_vector=False, block_norm=\"L2-Hys\")\n",
    "    ax[(2*i)%4, int(2*i/4)].imshow(hogimg)\n",
    "    ax[(2*i+1)%4, int(2*i/4)].imshow(img)\n",
    "    ax[(2*i)%4, int(2*i/4)].set_title(y_train[int(k)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of oriented gradients can be run on any image color channel. Because it can be somehow difficult to get the right tuning, a wrapper function is built in order to optimize for the parameter settings later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hog(img, orient, pix_per_cell, cell_per_block, feature_vec):\n",
    "    feature_array = hog(img, orientations=orient, pixels_per_cell=pix_per_cell, cells_per_block=cell_per_block, visualise=False, feature_vector=feature_vec, block_norm=\"L2-Hys\")\n",
    "    return feature_array\n",
    "\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, feature_vec=True, channel=0):\n",
    "    feature_image = img.copy()\n",
    "    if channel=='ALL':\n",
    "        feature_array = []\n",
    "        for i in range(3):\n",
    "            img = feature_image[:,:,i]\n",
    "            out_hog = calc_hog(img, orient, pix_per_cell, cell_per_block, feature_vec)\n",
    "            feature_array.append(out_hog)\n",
    "    else:\n",
    "        feature_array = [calc_hog(feature_image[:,:,channel], orient, pix_per_cell, cell_per_block, feature_vec)]\n",
    "    feature_array = np.asarray(feature_array)\n",
    "    return feature_array\n",
    "\n",
    "index_rand = np.random.uniform(0, x_train.shape[0], 10)\n",
    "for i, k in enumerate(index_rand):\n",
    "    img = x_train[int(k)]\n",
    "    feature_array = get_hog_features(img, orient=9, pix_per_cell=(8, 8), cell_per_block=(2, 2), feature_vec=False, channel=\"ALL\")\n",
    "    print(feature_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the hog features for different colorspaces and channels. The following parameters can be specified:\n",
    "\n",
    "* orient\n",
    "\n",
    "(5..21): Number of orientations which are calculated per cell\n",
    "* pix_per_cell\n",
    "\n",
    "(4,8,16): Number of pixels per cell which are calculated\n",
    "* cell_per_block\n",
    "\n",
    "(1, 2, 4, 8): Number of cells which are taken together to a block\n",
    "* colorspace\n",
    "\n",
    "(RGB, HSV, LUV, HLS, YUV, YCrCb): Colorspace for which HOG is calculated\n",
    "* channel\n",
    "\n",
    "(0,1,2,ALL): Which channel of the image is taken into account, even ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Pipeline\n",
    "To try this code, and to implement a basic pipeline, the hog features are used in order to build a very simple classifier to separate car from non-car images.\n",
    "This way, I can test the different possibilities for parameter optimization in the scikit library.\n",
    "\n",
    "In order to start off cleanly, we need to split the training data into a training array and a test array. This way we can make sure, that we are training and scoring our system properly. Also this functions shuffles the arrays for use, so that we can make sure, that we got an equal distribution of both classes in both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=rand_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our models, we want to have normalized data, so we are going to use the sklearn library to take care of the scaling for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat = []\n",
    "for image in X_train_split:\n",
    "    img_feat = get_hog_features(image, orient=9, pix_per_cell=(8,8), cell_per_block=(2,2), feature_vec=False, channel=\"ALL\")\n",
    "    X_train_feat.append(np.ravel(img_feat))\n",
    "X_train_feat = np.asarray(X_train_feat)\n",
    "print(X_train_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler().fit(X_train_feat)\n",
    "scaled_X_train = X_scaler.transform(X_train_feat)\n",
    "print(scaled_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_feat = []\n",
    "for image in X_test_split:\n",
    "    img_feat = get_hog_features(image, orient=9, pix_per_cell=(8,8), cell_per_block=(2,2), feature_vec=False, channel=\"ALL\")\n",
    "    X_test_feat.append(np.ravel(img_feat))\n",
    "X_test_feat = np.asarray(X_test_feat)\n",
    "print(X_test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to scale the test data similarly\n",
    "scaled_X_test = X_scaler.transform(X_test_feat)\n",
    "print(scaled_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to train a Support Vector Machine and try classifying our cars and non-cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(scaled_X_train, y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Accuracy of SVC = ', round(svc.score(scaled_X_test, y_test_split), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above test with SVC, we can reach an accuracy of 96%. This means, we have to get additional features to improve the identification rate.\n",
    "\n",
    "With 5200 features alone from HOG, we can test reducing the size  of the feature space, by reducing it to 1 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat = []\n",
    "for image in X_train_split:\n",
    "    img_feat = get_hog_features(image, orient=9, pix_per_cell=(8,8), cell_per_block=(2,2), feature_vec=False, channel=0)\n",
    "    X_train_feat.append(np.ravel(img_feat))\n",
    "X_train_feat = np.asarray(X_train_feat)\n",
    "print(X_train_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler().fit(X_train_feat)\n",
    "scaled_X_train = X_scaler.transform(X_train_feat)\n",
    "print(scaled_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_feat = []\n",
    "for image in X_test_split:\n",
    "    img_feat = get_hog_features(image, orient=9, pix_per_cell=(8,8), cell_per_block=(2,2), feature_vec=False, channel=0)\n",
    "    X_test_feat.append(np.ravel(img_feat))\n",
    "X_test_feat = np.asarray(X_test_feat)\n",
    "print(X_test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to scale the test data similarly\n",
    "scaled_X_test = X_scaler.transform(X_test_feat)\n",
    "print(scaled_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(scaled_X_train, y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Accuracy of SVC with reduced HOG features = ', round(svc.score(scaled_X_test, y_test_split), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, reduced HOG features don't reduce the accuracy that much, however the feature space is much smaller!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more Features\n",
    "\n",
    "In the next step I want to add more features to the pipeline. To start off, the color histogram can be a good indicator.\n",
    "\n",
    "Of course, colors itself are rather uninformative, however, using the Hue / Light / Saturation colorspace, we can focus on the Saturation channel. Cars tend to be in saturated colors, which should actually differentiate them from their surroundings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_histogram(image):\n",
    "    hhist = np.histogram(image[:,:,0], bins=64, range=(0, 256))\n",
    "    lhist = np.histogram(image[:,:,1], bins=64, range=(0, 256))\n",
    "    shist = np.histogram(image[:,:,2], bins=64, range=(0, 256))\n",
    "    return np.concatenate((hhist[0], lhist[0], shist[0]))\n",
    "\n",
    "index_rand = np.random.uniform(0, x_train.shape[0], 10)\n",
    "fig, ax = plt.subplots(4,5, figsize = (12,8))\n",
    "for i, k in enumerate(index_rand):\n",
    "    img = x_train[int(k)]\n",
    "    shist = get_color_histogram(img)\n",
    "    bin_centers = x = np.linspace(1, shist.shape[0], shist.shape[0])\n",
    "    ax[(2*i)%4, int(2*i/4)].bar(bin_centers, shist)\n",
    "    ax[(2*i+1)%4, int(2*i/4)].imshow(img)\n",
    "    ax[(2*i)%4, int(2*i/4)].set_title(y_train[int(k)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the saturation histogram of different cars and non-cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step I also want to introduce spatial binning of the image. In this way we are going to reduce the image size and input ALL the pixels to the training algorithm. This way we can put all the color information in the algorithm too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_spacial_binning(image, size=(16,16)):\n",
    "    small_img = cv2.resize(image, size)\n",
    "    return small_img.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing special to show here, all the image information is just put into a whole vector. As a next step, we can use our features to put all of them in a new feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_feature_vector(image):\n",
    "    color_space=\"LUV\"\n",
    "    if color_space == 'HSV':\n",
    "        feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    elif color_space == 'LUV':\n",
    "        feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "    elif color_space == 'HLS':\n",
    "        feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "    elif color_space == 'YUV':\n",
    "        feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "    elif color_space == 'YCrCb':\n",
    "        feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "    else:\n",
    "        feature_image = image.copy()\n",
    "\n",
    "    # Get HOG features of the image\n",
    "    hog = get_hog_features(feature_image, orient=9, pix_per_cell=(16,16), cell_per_block=(2, 2), feature_vec=False, channel=\"ALL\")\n",
    "    # Get Color Histogram of the image\n",
    "    hist = get_color_histogram(feature_image)\n",
    "    # Get Spacial Binning of the image\n",
    "    binning = get_spacial_binning(feature_image, size=(8,8))\n",
    "    \n",
    "    features = np.concatenate((np.ravel(hog), np.ravel(hist), np.ravel(binning)))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_rand = int(np.random.uniform(0, x_train.shape[0], 1))\n",
    "img = x_train[index_rand]\n",
    "\n",
    "feature = generate_feature_vector(img)\n",
    "x = np.linspace(1, feature.shape[0], feature.shape[0])\n",
    "\n",
    "plt.plot(x, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the feature space is rather unbalanced. To overcome this, we will normalize it using the helper tool from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_rand = np.random.uniform(0, x_train.shape[0], 10)\n",
    "features_x = []\n",
    "for i, k in enumerate(index_rand):\n",
    "    img = x_train[int(k)]\n",
    "    feature = generate_feature_vector(img)\n",
    "    features_x.append(feature)\n",
    "\n",
    "x_test_scaler = StandardScaler().fit(features_x)\n",
    "example_feature = x_test_scaler.transform(features_x)\n",
    "print(example_feature.shape)\n",
    "\n",
    "plt.plot(x, example_feature[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see after the scaling, the Feature Vector looks much more unified. Now we can apply this to the whole dataset, and let SKLearn train on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat = []\n",
    "for image in X_train_split:\n",
    "    img_feat = generate_feature_vector(image)\n",
    "    X_train_feat.append(np.ravel(img_feat))\n",
    "X_train_feat = np.asarray(X_train_feat)\n",
    "print(X_train_feat.shape)\n",
    "\n",
    "X_scaler = StandardScaler().fit(X_train_feat)\n",
    "scaled_X_train = X_scaler.transform(X_train_feat)\n",
    "print(scaled_X_train.shape)\n",
    "\n",
    "X_test_feat = []\n",
    "for image in X_test_split:\n",
    "    img_feat = generate_feature_vector(image)\n",
    "    X_test_feat.append(np.ravel(img_feat))\n",
    "X_test_feat = np.asarray(X_test_feat)\n",
    "print(X_test_feat.shape)\n",
    "\n",
    "# Make sure to scale the test data similarly\n",
    "scaled_X_test = X_scaler.transform(X_test_feat)\n",
    "print(scaled_X_test.shape)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(scaled_X_train, y_train_split)\n",
    "\n",
    "print('Test Accuracy of SVC with multiple features = ', round(svc.score(scaled_X_test, y_test_split), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the first run I could achieve an Accuracy of 99.25% using the follwing parameters: \n",
    "\n",
    "* HOG orient 9\n",
    "* HOG pixel per cell 16\n",
    "* HOG cells per block 2\n",
    "* Saturation Histogram of HLS space, 64 bins\n",
    "* 8, 8 binned image\n",
    "\n",
    "Let's see if we can optimize the parameters for the Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "\n",
    "svr = svm.SVC()\n",
    "svr.fit(scaled_X_train, y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Accuracy of nonlinear SVC with all features = ', round(svr.score(scaled_X_test, y_test_split), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Accuracy of nonlinear SVC with all features = ', round(svr.score(scaled_X_train, y_train_split), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "\n",
    "svr = svm.SVC(C = 100.0)\n",
    "svr.fit(scaled_X_train, y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Accuracy of nonlinear SVC with all features = ', round(svr.score(scaled_X_test, y_test_split), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(C = 100.0)\n",
    "svc.fit(scaled_X_train, y_train_split)\n",
    "\n",
    "print('Test Accuracy of SVC with multiple features = ', round(svc.score(scaled_X_test, y_test_split), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, another possibility would be to train an alternative classifier, such as a neural network. In this case, I will use a fully connected network to show the accuracy difference between SVC and the neural network. In the end I will make a small speed comparison, becasue the classifier still has to run in real time on the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, Dropout, Lambda\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(1164,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "one_hot_labels = to_categorical(y_train_split)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(scaled_X_train, one_hot_labels, validation_split=0.2, shuffle=True, epochs=4, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(model.predict(scaled_X_test)[:,1]-y_test_split))/np.size(y_test_split) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the keras model was able to generalize well on the dataset, having only 1% error on the final test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Benchmark\n",
    "\n",
    "To benchmark our models, we need to calculate the time investment for each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index = 50\n",
    "test_image = X_test_split[index]\n",
    "test_result = y_test_split[index]\n",
    "\n",
    "start_time = time.time()\n",
    "feature = generate_feature_vector(test_image)\n",
    "scaled_feature = X_scaler.transform(feature)\n",
    "result = svc.predict(scaled_feature)\n",
    "time_svc = time.time() - start_time\n",
    "print(time_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "feature = generate_feature_vector(test_image)\n",
    "scaled_feature = X_scaler.transform(feature)\n",
    "result = svr.predict(scaled_feature)\n",
    "time_svc = time.time() - start_time\n",
    "print(time_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The svc model shows a good accuracy combined with a very fast execution time. It clearly stands out compared to the neural network due to its high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window on Images\n",
    "\n",
    "In the next step after I trained my classifier I had a look at the training images to run it on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_folder = \"C:/Users/Marc Engeler/PycharmProjects/Car_Tracking_P5/test_images\"\n",
    "test_folder = \"/user_storage/meng/Car_Tracking_P5/test_images\"\n",
    "new_files = [join(test_folder, f) for f in listdir(test_folder) if isfile(join(test_folder, f))]\n",
    "test_images = []\n",
    "for file in new_files:\n",
    "    img = cv2.imread(file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    test_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3)\n",
    "for i in range(6):\n",
    "    img = test_images[i]\n",
    "    ax[i%2, int(i/2)].imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we define a region of interest, which will define the search are where we want to find cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = (1, 380)\n",
    "x2 = (1280, 719)\n",
    "\n",
    "img = test_images[3].copy()\n",
    "cv2.rectangle(img, x0, x2, (0,255,0), 10)\n",
    "plt.imshow(img)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test the cropped car with our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = (400,830)\n",
    "x1 = (470,940)\n",
    "\n",
    "img_cropped = img[x0[0]:x1[0], x0[1]:x1[1]]\n",
    "img_cropped = cv2.resize(img_cropped, (64, 64))\n",
    "features = generate_feature_vector(img_cropped)\n",
    "scaled_feature = X_scaler.transform(features)\n",
    "result = svr.predict(scaled_feature)\n",
    "\n",
    "x = np.linspace(1, scaled_feature.shape[0], scaled_feature.shape[0])\n",
    "plt.plot(x, scaled_feature)\n",
    "plt.show()\n",
    "print(result)\n",
    "plt.imshow(img_cropped)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As next appraoch, the sliding window function is imlemented. To do so multiple sliding window sizes have to be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    x_start_stop = np.array(x_start_stop)\n",
    "    y_start_stop = np.array(y_start_stop)\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\n",
    "    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\n",
    "    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \n",
    "    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list\n",
    "\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the computational road, small sliding windows are only calculated further up in the image, where small cards are to be expected, the closer we get to the car, the larger the sliding windows are.\n",
    "\n",
    "Using such a separation in distance and size of the windows allows us to run less windows over each image. In the end the computational load increases with each window that we have in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [(64, 64), (96, 96), (128, 128), (256, 256)]\n",
    "ylims = [(360, 500), (380, 580), (380, 600), (450, 720)]\n",
    "overlap = [(0.75, 0.75), (0.78, 0.78), (0.8, 0.8), (0.8, 0.8)]\n",
    "colors = [(255, 255, 255), (0, 0, 255), (0, 255, 0), (255, 0, 0), (255, 0, 255), (0, 255, 255)]\n",
    "windows_arr = []\n",
    "window_img = test_images[2].copy()\n",
    "for i, size in enumerate(sizes):\n",
    "    windows = slide_window(img, x_start_stop=(None, None), y_start_stop=ylims[i], xy_window=size, xy_overlap=overlap[i])\n",
    "    window_img = draw_boxes(window_img, windows, color=colors[i], thick=6)                    \n",
    "plt.imshow(window_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a pipeline for an image, to correctly select the sliding window, and crop it to its according size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sizes = [(64, 64), (96, 96), (128, 128), (256, 256)]\n",
    "ylims = [(380, 500), (400, 560), (430, 600), (450, 720)]\n",
    "overlap = [(0.75, 0.75), (0.78, 0.78), (0.8, 0.8), (0.8, 0.8)]\n",
    "def get_car_heatmap(image):\n",
    "    heat = np.zeros_like(image[:,:,0])\n",
    "    res_window = []\n",
    "    for i, size in enumerate(sizes):\n",
    "        windows = slide_window(img, x_start_stop=(None, None), y_start_stop=ylims[i], \n",
    "                    xy_window=size, xy_overlap=overlap[i])\n",
    "        for window in windows:\n",
    "            cropped = cv2.resize(image[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64,64))\n",
    "            features = generate_feature_vector(cropped)\n",
    "            scaled_feature = X_scaler.transform(features)\n",
    "            result = svc.predict(scaled_feature)\n",
    "            if result == 1:\n",
    "                heat[window[0][1]:window[1][1], window[0][0]:window[1][0]] += 1\n",
    "                res_window.append(window)\n",
    "    return heat, res_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = test_images[0].copy()\n",
    "heatmap, res_windows = get_car_heatmap(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "labels = label(heatmap >= 5)\n",
    "print(labels[0].shape)\n",
    "plt.imshow(np.float32(labels[0]))\n",
    "plt.show()\n",
    "\n",
    "print(labels[1], 'cars found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Car Pipeline\n",
    "\n",
    "In order to fluently identify cars in images, we can define a pipeline, based on a simple nearest neighbor calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def find_cars(labels, feature_num):\n",
    "    cars = []\n",
    "    boxes = []\n",
    "    for car_number in range(1, feature_num+1):\n",
    "        nonzero = (labels == car_number).nonzero()\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        pos = (int(np.mean(nonzerox)), int(np.mean(nonzeroy)))\n",
    "        cars.append(pos)\n",
    "        xmin = np.min(nonzerox)\n",
    "        xmax = np.max(nonzerox)\n",
    "        ymin = np.min(nonzeroy)\n",
    "        ymax = np.max(nonzeroy)\n",
    "        boxes.append(((xmin, ymin), (xmax, ymax)))\n",
    "    return cars, boxes\n",
    "    \n",
    "\n",
    "   \n",
    "def generate_feature_vector_PL(image):\n",
    "    # Get HOG features of the image\n",
    "    hog = get_hog_features(image, orient=9, pix_per_cell=(16, 16), cell_per_block=(2, 2), feature_vec=False, channel=\"ALL\")\n",
    "    # Get Color Histogram of the image\n",
    "    hist = get_color_histogram(image)\n",
    "    # Get Spacial Binning of the image\n",
    "    binning = get_spacial_binning(image, size=(8,8))\n",
    "    \n",
    "    features = np.concatenate((np.ravel(hog), np.ravel(hist), np.ravel(binning)))\n",
    "\n",
    "    return features\n",
    "\n",
    "positions_old = []\n",
    "history = deque(maxlen = 8)\n",
    "\n",
    "def get_car_heatmap(img):\n",
    "    global positions_old\n",
    "    global history\n",
    "    image = img.copy()\n",
    "    \n",
    "    heat = np.zeros_like(image[:,:,0])\n",
    "    feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "    \n",
    "    for i, size in enumerate(sizes):\n",
    "        '''\n",
    "        img_cropped = feature_image[ylims[i][0]:ylims[i][1], :]\n",
    "        pix_per_cell = int(16 * size[0]/64)\n",
    "        hog_cropped_total = get_hog_features(img_cropped, orient=9, pix_per_cell=(pix_per_cell, pix_per_cell), cell_per_block=(2, 2),\n",
    "                                 feature_vec=False, channel=\"ALL\")\n",
    "        \n",
    "        total_steps_x = int(image.shape[1]/size[0]) * 2\n",
    "        total_steps_y = int((ylims[i][1]-ylims[i][0])/size[1]) * 2\n",
    "        '''\n",
    "        windows = slide_window(feature_image, x_start_stop=(None, None), y_start_stop=ylims[i], \n",
    "                    xy_window=size, xy_overlap=overlap[i])\n",
    "        \n",
    "        for i, window in enumerate(windows):\n",
    "            '''\n",
    "            xpos = i%total_steps_x\n",
    "            ypos = int(i/total_steps_x)\n",
    "            hog_feat1 = hog_cropped_total[0][ypos:ypos+3, xpos:xpos+3].ravel()\n",
    "            hog_feat2 = hog_cropped_total[1][ypos:ypos+3, xpos:xpos+3].ravel() \n",
    "            hog_feat3 = hog_cropped_total[2][ypos:ypos+3, xpos:xpos+3].ravel() \n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "            '''\n",
    "            cropped = cv2.resize(feature_image[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64,64))\n",
    "            features = generate_feature_vector_PL(cropped)\n",
    "            scaled_feature = X_scaler.transform(features)\n",
    "            result = svr.predict(scaled_feature)\n",
    "            if result == 1:\n",
    "                heat[window[0][1]:window[1][1], window[0][0]:window[1][0]] += 1\n",
    "    history.append(heat)\n",
    "    current_heat_map = np.zeros_like(heat)\n",
    "    for current in history:\n",
    "        current_heat_map += current\n",
    "\n",
    "    labels, feature_num = label(current_heat_map>=20)\n",
    "    positions, boxes = find_cars(labels, feature_num)\n",
    "\n",
    "    \n",
    "    printOK = []\n",
    "    for position in positions:\n",
    "        dx = [(position[0]-x[0])**2 for x in positions_old]\n",
    "        dy = [(position[1]-y[1])**2 for y in positions_old]\n",
    "        try:\n",
    "            ok_print = np.min(dx + dy)<5000\n",
    "            printOK.append(ok_print)\n",
    "        except:\n",
    "            printOK.append(False)\n",
    "        \n",
    "    for i, box in enumerate(boxes):\n",
    "        if printOK[i]:\n",
    "            bsizex = np.abs(box[1][0]-box[0][0])\n",
    "            bsizey = np.abs(box[1][1]-box[0][1])\n",
    "            if (bsizex > 64) & (bsizey > 64):\n",
    "                cropped = cv2.resize(feature_image[box[0][1]:box[1][1], box[0][0]:box[1][0]], (64,64))\n",
    "                features = generate_feature_vector_PL(cropped)\n",
    "                scaled_feature = X_scaler.transform(features)\n",
    "                result = svr.predict(scaled_feature)\n",
    "                if result:\n",
    "                    image = cv2.rectangle(image, box[0], box[1], (0,0,255), 6)\n",
    "    positions_old = positions\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    img = test_images[0].copy()\n",
    "    tmp = get_car_heatmap(img.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_car_heatmap(img.copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "vidout = 'output.mp4'\n",
    "vidin = VideoFileClip('project_video.mp4')\n",
    "processed_video = vidin.fl_image(get_car_heatmap)\n",
    "%time processed_video.write_videofile(vidout, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "To summarize this pipeline, I think that the svc classifier is a bit too slow. Also the HOG sampling needs a lot of calculation, even with the global method provided in the lessons, with the 4 different sampling window sizes, it still requires 4 HOG over the whole image, which I deem rather inefficient.\n",
    "\n",
    "In this case, I would probably use a fast convolutional network trained to identify pedestrians and cars in an image. Systems such as darknet can run really fast on embedded devices and are perfectly applicable for such an application.\n",
    "\n",
    "All in all it's still a nice insight into the identification of cars in images, and how a systematic approach looks like.\n",
    "\n",
    "I think the biggest weakness of this algorithm is still it's hyperparameter space, which are dozens of parameters to tune in the end. Also the calculations based on certain colorspaces may be uncertain for rainy or foggy conditions, which are much easier to train in a neural network or similar architecture.\n",
    "\n",
    "Unfortunately, I didn't get the hog-algorithm to run more efficiently when compiled on the whole image, although I calculated it only on the area of interest each time, the code was up to 3 times slower than when compiled on every single frame."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
